<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>The LogSumExp trick &mdash; Parsiad Azimzadeh</title>
  <link href="/assets/main.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Blog &mdash; Parsiad Azimzadeh" type="application/atom+xml">
  <link crossorigin href="https://fonts.googleapis.com" rel="preconnect">
  <link crossorigin href="https://fonts.gstatic.com" rel="preconnect">
  <link crossorigin href="https://fonts.googleapis.com/css2?family=EB+Garamond&family=Source+Code+Pro&display=swap" rel="stylesheet">
  <link crossorigin href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css" integrity="sha384-uo/9/s/Ns8DTg4kjkjex8GezUcgMlKD99gTqxvMkIsaG4lSUbeJ0dVELljipv94t" rel="stylesheet">
  <link crossorigin href="https://cdn.jsdelivr.net/npm/katex@0.16.27/dist/katex.min.css" integrity="sha384-Pu5+C18nP5dwykLJOhd2U4Xen7rjScHN/qusop27hdd2drI+lL5KvX7YntvT8yew" rel="stylesheet">
  <link crossorigin href="https://cdn.jsdelivr.net/npm/glightbox@3.3.1/dist/css/glightbox.min.css" integrity="sha384-GPAzSuZc0kFvdIev6wm9zg8gnafE8tLso7rsAYQfc9hAdWCpOcpcNI5W9lWkYcsd" rel="stylesheet">
  <link crossorigin rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/base16/equilibrium-gray-light.min.css">
  <script src="/assets/main.js"></script>
  <script type="module" src="/assets/module.js"></script>
  <script crossorigin defer integrity="sha384-2B8pfmZZ6JlVoScJm/5hQfNS2TI/6hPqDZInzzPc8oHpN5SgeNOf4LzREO6p5YtZ" src="https://cdn.jsdelivr.net/npm/katex@0.16.27/dist/katex.min.js"></script>
  <script crossorigin defer integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.27/dist/contrib/auto-render.min.js"></script>
  <script crossorigin integrity="sha384-MZZbZ6RXJudK43v1qY1zOWKOU2yfeBPatuFoKyHAaAgHTUZhwblRTc9CphTt4IGQ" src="https://cdn.jsdelivr.net/npm/glightbox@3.3.1/dist/js/glightbox.min.js"></script>
</head>

<body>
  <h1>Parsiad Azimzadeh</h1>
  <nav>
    <ul>
      <li><a href="/">About</a></li><li><a href="/blog/">Blog</a></li><li><a href="/pubs/">Publications</a></li><li><a href="/code/">Code</a></li><li><a href="/sols/">Solutions</a></li>
    </ul>
  </nav>
  <div id="content">
<h1 class="title">The LogSumExp trick</h1>
<time datetime="2023-12-17">December 17, 2023</time>
<h2 id="motivation">Motivation</h2>
<p>The <a rel="external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> $\sigma$ is used to transform a vector in $\mathbb{R}^n$ to a probability vector in a monotonicity-preserving way.
Specifically, if $x_i \leq x_j$, then $\sigma(x)_i \leq \sigma(x)_j$.</p>
<p>The softmax is typically parametrized by a "temperature" parameter $T$ to yield $\sigma_T(x) \equiv \sigma(x / T)$ which</p>
<ul>
<li>shifts more probability mass to the largest component of $x$ as the temperature decays to zero and</li>
<li>distributes the mass more evenly among the components of $x$ as the temperature grows.</li>
</ul>
<p>More details regarding the temperature can be found in <a href="/blog/softmax-sensitivity-to-temperature">a previous blog post</a>.</p>
<p>Algebraically, the softmax is defined as</p>
<p>$$
\sigma(x)_i \equiv \frac{\exp(x_i)}{\sum_j \exp(x_j)}.
$$</p>
<p>This quantity is clearly continuous on $\mathbb{R}^n$ and hence finite there.
However, in the presence of floating point computation, computing this quantity naively can result in blow-up:</p>
<pre><code data-lang="python">x = np.array([768, 1024.0])
exp_x = np.exp(x)
exp_x / exp_x.sum()
</code></pre>
<pre><code>/tmp/ipykernel_7346/152377539.py:2: RuntimeWarning: overflow encountered in exp
  exp_x = np.exp(x)
/tmp/ipykernel_7346/152377539.py:3: RuntimeWarning: invalid value encountered in divide
  exp_x / exp_x.sum()





array([nan, nan])
</code></pre>
<p>The <em>LogSumExp trick</em> is a clever way of reformulating this computation so that it is robust to floating point error.</p>
<h2 id="the-logsumexp-trick">The LogSumExp trick</h2>
<p>First, let $\bar{x} = \max_i x_i$ and note that</p>
<p>$$
\sigma(x)_{i}=\frac{\exp(x_{i}-\bar{x})}{\sum_{j}\exp(x_{j}-\bar{x})}.
$$</p>
<p>Taking logarithms,</p>
<p>$$
\log(\sigma(x)_{i})=x_{i}-\bar{x}-\log\biggl(\sum_{j}\exp(x_{j}-\bar{x})\biggr).
$$</p>
<p>Exponentiating,</p>
<p>$$
\sigma(x)_{i}=\exp\biggr(x_{i}-\bar{x}-\log\biggl(\sum_{j}\exp(x_{j}-\bar{x})\biggr)\biggr).
$$</p>
<p>In particular, note that $x_j - \bar{x}$ is, by construction, nonpositive and hence has a value less than one when exponentiated.</p>
<pre><code data-lang="python">def softmax(x: np.ndarray) -&gt; np.ndarray:
    x_max = x.max(axis=-1, keepdims=True)
    delta = x - x_max
    lse = np.log(np.exp(delta).sum(axis=-1, keepdims=True))
    return np.exp(delta - lse)
</code></pre>
<pre><code data-lang="python">x = np.array([768, 1024.0])
softmax(x)
</code></pre>
<pre><code>array([6.61626106e-112, 1.00000000e+000])
</code></pre>

</div>
  <footer>Powered by <a href="https://getzola.org/">Zola</a></footer>
</body>

</html>