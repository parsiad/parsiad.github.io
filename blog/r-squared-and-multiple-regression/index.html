<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>R squared and multiple regression &mdash; Parsiad Azimzadeh</title>
  <link href="/assets/main.css" rel="stylesheet">
  <link href="/atom.xml" rel="alternate" title="Blog &mdash; Parsiad Azimzadeh" type="application/atom+xml">
  <link crossorigin href="https://fonts.googleapis.com" rel="preconnect">
  <link crossorigin href="https://fonts.gstatic.com" rel="preconnect">
  <link crossorigin href="https://fonts.googleapis.com/css2?family=EB+Garamond&family=Source+Code+Pro&display=swap" rel="stylesheet">
  <link crossorigin href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css" integrity="sha384-uo/9/s/Ns8DTg4kjkjex8GezUcgMlKD99gTqxvMkIsaG4lSUbeJ0dVELljipv94t" rel="stylesheet">
  <link crossorigin href="https://cdn.jsdelivr.net/npm/katex@0.16.27/dist/katex.min.css" integrity="sha384-Pu5+C18nP5dwykLJOhd2U4Xen7rjScHN/qusop27hdd2drI+lL5KvX7YntvT8yew" rel="stylesheet">
  <link crossorigin href="https://cdn.jsdelivr.net/npm/glightbox@3.3.1/dist/css/glightbox.min.css" integrity="sha384-GPAzSuZc0kFvdIev6wm9zg8gnafE8tLso7rsAYQfc9hAdWCpOcpcNI5W9lWkYcsd" rel="stylesheet">
  <link crossorigin rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/base16/equilibrium-gray-light.min.css">
  <script src="/assets/main.js"></script>
  <script type="module" src="/assets/module.js"></script>
  <script crossorigin defer integrity="sha384-2B8pfmZZ6JlVoScJm/5hQfNS2TI/6hPqDZInzzPc8oHpN5SgeNOf4LzREO6p5YtZ" src="https://cdn.jsdelivr.net/npm/katex@0.16.27/dist/katex.min.js"></script>
  <script crossorigin defer integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.27/dist/contrib/auto-render.min.js"></script>
  <script crossorigin integrity="sha384-MZZbZ6RXJudK43v1qY1zOWKOU2yfeBPatuFoKyHAaAgHTUZhwblRTc9CphTt4IGQ" src="https://cdn.jsdelivr.net/npm/glightbox@3.3.1/dist/js/glightbox.min.js"></script>
</head>

<body>
  <h1>Parsiad Azimzadeh</h1>
  <nav>
    <ul>
      <li><a href="/">About</a></li><li><a href="/blog/">Blog</a></li><li><a href="/pubs/">Publications</a></li><li><a href="/code/">Code</a></li><li><a href="/sols/">Solutions</a></li>
    </ul>
  </nav>
  <div id="content">
<h1 class="title">R squared and multiple regression</h1>
<time datetime="2024-09-02">September 2, 2024</time>
<h2 id="introduction-and-results">Introduction and Results</h2>
<p>The <em>R squared</em> of a predictor $\hat{Y}$ relative to a target $Y$ is the proportion of the variation in the target that is explained by that predictor.
In this short note, we introduce the R squared in its most general form.</p>
<p>We then turn our attention to the predictor $\hat{Y}_{\mathrm{ols}} = \hat{\beta}_0 + X^\intercal \hat{\beta}_1$ constructed by <a rel="external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a> (OLS).
In this case, we prove that the R squared satisfies</p>
<p>$$
\boxed{R^{2}(Y,\hat{Y}_{\mathrm{ols}})
=\frac{\operatorname{Var}(\hat{Y}_{\mathrm{ols}})}{\operatorname{Var}(Y)}
=\frac{\operatorname{Cov}(Y,\hat{Y}_{\mathrm{ols}})}{\operatorname{Var}(Y)}
=\frac{\operatorname{Cov}(X,Y)^\intercal \hat{\beta}_1}{\operatorname{Var}(Y)}
=\operatorname{Corr}(Y,\hat{Y}_{\mathrm{ols}})^{2}}
$$</p>
<p>where the covariance $\operatorname{Cov}(X, Y)$ is the vector whose entries are $\operatorname{Cov}(X_i, Y)$.</p>
<p>In addition, when the covariance matrix $\operatorname{Cov}(X, X)$ is nonsingular, we can substitute the unique representation for $\hat{\beta}_1$ to get</p>
<p>$$
\boxed{R^{2}(Y,\hat{Y}_{\mathrm{ols}})
=\frac{\operatorname{Cov}(X,Y)^\intercal\operatorname{Cov}(X,X)^{-1}\operatorname{Cov}(X,Y)}{\operatorname{Var}(Y)}.}
$$</p>
<p>In the case of a one dimensional $X$, the above is the square of the <a rel="external" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation</a> between $X$ and $Y$.</p>
<p>These identities may be folklore in the following sense: I believe they are well-known but that a proof, outside of the one dimensional case, is hard to find.
Note that in the one dimensional case, OLS (with the intercept term $\hat{\beta}_0$) is simply called <a rel="external" href="https://en.wikipedia.org/wiki/Simple_linear_regression">simple linear regression</a>.</p>
<h2 id="r-squared">R squared</h2>
<p>As mentioned above, the R squared is a fraction of variation.
A natural notion of variation between two random quantities is the mean squared error:</p>
<p><strong>Definition (Mean squared error).</strong>
The <em>mean squared error</em> (MSE) between real random variables $A$ and $B$ is</p>
<p>$$
\operatorname{MSE}(A, B) \equiv \mathbb{E}[(A - B)^2].
$$</p>
<p>We are now ready to define R squared:</p>
<p><strong>Definition (R squared).</strong>
Let $Y$ and $\hat{Y}$ be real random variables. The <em>R squared</em> of $\hat{Y}$ relative to $Y$ is</p>
<p>$$
R^2(Y, \hat{Y})
\equiv 1 - \frac{\operatorname{MSE}(Y,\hat{Y})}{\operatorname{Var}(Y)}
$$</p>
<p>Written in the above form, the R squared has an intuitive definition: it is equal to one minus the fraction of unexplained variation (note that the variance itself is a special case of the MSE since $\operatorname{Var}(Y) = \operatorname{MSE}(Y,\mathbb{E}Y)$).</p>
<p>The following facts can be verified by direct substitution into the definition of R squared:</p>
<p><strong>Fact.</strong>
<em>A "perfect" prediction (i.e., $\operatorname{MSE}(Y,\hat{Y})=0$) has an R squared of one.</em></p>
<p><strong>Fact.</strong>
<em>A prediction of the mean (i.e., $\hat{Y}=\mathbb{E}Y)$ has an R squared of zero.</em></p>
<p>The above give us an upper bound (one) and a "weak" lower bound (zero) for the R squared.
The lower bound is weak in the sense that it is possible to produce predictors that have a negative R squared but that such predictors are typically pathological as they are worse (in the sense of R squared) than simply predicting the mean.</p>
<h2 id="ordinary-least-squares">Ordinary least squares</h2>
<p>Below, we define OLS with an intercept term.</p>
<p><strong>Definition (Ordinary least squares).</strong>
Given a real random variable $Y$ and a real random vector $X$, let</p>
<p>$$
\hat{Y}_{\mathrm{ols}}\equiv\hat{\beta}_{0}+X^{\intercal}\hat{\beta}_{1}
$$</p>
<p>be the <em>ordinary least squares</em> (OLS) predictor where</p>
<p>$$
\begin{aligned}
\hat{\beta}_{0} &amp; =\mathbb{E}Y-\mathbb{E}X^{\intercal}\hat{\beta}_{1}\nonumber \\
\mathbb{E}[XX^{\intercal}]\hat{\beta}_{1} &amp; =\mathbb{E}[XY]-\mathbb{E}X\hat{\beta}_{0}.
\end{aligned}
$$</p>
<p><em>Remark</em>.
We could have also defined $\hat{\beta}_0$ and $\hat{\beta}_1$ by the equivalent system</p>
<p>$$
\begin{aligned}
\hat{\beta}_{0} &amp; =\mathbb{E}Y-\mathbb{E}X^{\intercal}\hat{\beta}_{1}\nonumber \\
\operatorname{Cov}(X, X)\hat{\beta}_{1} &amp; =\operatorname{Cov}(X, Y).
\end{aligned}
$$</p>
<p><em>Remark</em>.
Thus far, we have only used a single probability measure: that which is implied by the expectation $\mathbb{E}$.
In data-driven applications, it is standard practice to fit the coefficients on a subset of data (also known as the <a rel="external" href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Training_data_set">training set</a>) while "holding out" the remainder of the data (also known as the <a rel="external" href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Test_data_set">test set</a>) to evaluate the quality of the fit.
This results in two distinct expectations: $\mathbb{E}$ and $\mathbb{E}_H$ ($H$ for "hold out").
In the context of R squared, this results in two natural quantities: $R^2$ and $R_H^2$ where the latter is defined by replacing all expectations $\mathbb{E}$ in the MSE and variance by $\mathbb{E}_H$.
We stress that the results of this section apply only to the former.
Put more succinctly, <strong>on the test set, the R squared is not to satisfy the identities</strong> listed at the beginning of this article.</p>
<p>To establish that on the training set, the R squared satisfies the identities listed at the beginning of this article, we use a series of smaller results:</p>
<p><strong>Lemma 1.</strong>
<em>The OLS predictor is unbiased.</em></p>
<p><em>Proof</em>.
Direct computation yields</p>
<p>$$
\mathbb{E}\hat{Y}_{\mathrm{ols}}\equiv\hat{\beta}_{0}+\mathbb{E}X^{\intercal}\hat{\beta}_{1}=\mathbb{E}Y-\mathbb{E}X^{\intercal}\hat{\beta}_{1}+\mathbb{E}X^{\intercal}\hat{\beta}_{1}=\mathbb{E}Y.\blacksquare
$$</p>
<p><strong>Lemma 2.</strong>
<em>The second moment of the OLS predictor satisfies</em></p>
<p>$$
\mathbb{E}[\hat{Y}_{\mathrm{ols}}^{2}]
= \mathbb{E}[Y\hat{Y}_{\mathrm{ols}}]
= \left(\mathbb{E}Y\right)^2 + \operatorname{Cov}(X, Y)^\intercal \hat{\beta}_1.
$$</p>
<p><em>Proof</em>.
Direct computation along with the definitions of $\hat{\beta}_0$ and $\mathbb{E}[XX^{\intercal}]\hat{\beta}_1$ reveal</p>
<p>$$
\begin{aligned}
\mathbb{E}[Y\hat{Y}_{\mathrm{ols}}] &amp; =\mathbb{E}\left[Y\left(\hat{\beta}_{0}+X^{\intercal}\hat{\beta}_{1}\right)\right]\\
&amp; =\mathbb{E}\left[Y\hat{\beta}_{0}+X^{\intercal}Y\hat{\beta}_{1}\right]\\
&amp; =\mathbb{E}Y\hat{\beta}_{0}+\mathbb{E}[X^{\intercal}Y]\hat{\beta}_{1}\\
&amp; =\mathbb{E}Y\left(\mathbb{E}Y-\mathbb{E}X^{\intercal}\hat{\beta}_{1}\right)+\mathbb{E}[X^{\intercal}Y]\hat{\beta}_{1}\\
&amp; =\left(\mathbb{E}Y\right)^{2}-\mathbb{E}X^{\intercal}\mathbb{E}Y\hat{\beta}_{1}+\mathbb{E}[X^{\intercal}Y]\hat{\beta}_{1}
\end{aligned}
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
\mathbb{E}[\hat{Y}_{\mathrm{ols}}^{2}] &amp; =\mathbb{E}\left[\left(\hat{\beta}_{0}+X^{\intercal}\hat{\beta}_{1}\right)^{2}\right]\\
&amp; =\mathbb{E}\left[\hat{\beta}_{0}^{2}+2\hat{\beta}_{0}X^{\intercal}\hat{\beta}_{1}+\hat{\beta}_{1}^{\intercal}XX^{\intercal}\hat{\beta}_{1}\right]\\
&amp; =\hat{\beta}_{0}^{2}+2\hat{\beta}_{0}\mathbb{E}X^{\intercal}\hat{\beta}_{1}+\hat{\beta}_{1}^{\intercal}\mathbb{E}[XX^{\intercal}]\hat{\beta}_{1}\\
&amp; =\hat{\beta}_{0}^{2}+2\hat{\beta}_{0}\mathbb{E}X^{\intercal}\hat{\beta}_{1}+\left(\mathbb{E}[X^{\intercal}Y]-\hat{\beta}_{0}\mathbb{E}X^{\intercal}\right)\hat{\beta}_{1}\\
&amp; =\left(\mathbb{E}Y-\mathbb{E}X^{\intercal}\hat{\beta}_{1}\right)^{2}+\left(\mathbb{E}Y-\mathbb{E}X^{\intercal}\hat{\beta}_{1}\right)\mathbb{E}X^{\intercal}\hat{\beta}_{1}+\mathbb{E}\left[X^{\intercal}Y\right]\hat{\beta}_{1}\\
&amp; =\left(\mathbb{E}Y\right)^{2}-\mathbb{E}X^{\intercal}\mathbb{E}Y\hat{\beta}_{1}+\mathbb{E}[X^{\intercal}Y]\hat{\beta}_{1}
\end{aligned}
$$</p>
<p>as desired. $\blacksquare$</p>
<p><strong>Corollary 3.</strong>
<em>The variance of the OLS predictor satisfies</em></p>
<p>$$
\operatorname{Var}(\hat{Y}_{\mathrm{ols}})
=\operatorname{Cov}(Y,\hat{Y}_{\mathrm{ols}}).
$$</p>
<p><em>Proof</em>.
Applying Lemmas 1 and 2,</p>
<p>$$
\operatorname{Cov}(Y,\hat{Y}_{\mathrm{ols}})=\mathbb{E}[Y\hat{Y}_{\mathrm{ols}}]-\mathbb{E}Y\mathbb{E}\hat{Y}_{\mathrm{ols}}=\mathbb{E}[\hat{Y}_{\mathrm{ols}}^{2}]-\left(\mathbb{E}\hat{Y}_{\mathrm{ols}}\right)^{2}=\operatorname{Var}(\hat{Y}_{\mathrm{ols}}).\blacksquare
$$</p>
<p><strong>Corollary 4.</strong>
<em>The MSE of the OLS predictor can be decomposed as</em></p>
<p>$$
\operatorname{MSE}(Y,\hat{Y}_{\mathrm{ols}})
=\operatorname{Var}(Y)-\operatorname{Var}(\hat{Y}_{\mathrm{ols}}).
$$</p>
<p><em>Proof</em>.
Applying Lemmas 1 and 2 and Corollary 3,</p>
<p>$$
\begin{aligned}
\operatorname{Var}(Y)-\operatorname{MSE}(Y,\hat{Y}_{\mathrm{ols}}) &amp; =\mathbb{E}[Y^{2}]-\left(\mathbb{E}Y\right)^{2}-\mathbb{E}[Y^{2}]+2\mathbb{E}[Y\hat{Y}_{\mathrm{ols}}]-\mathbb{E}[\hat{Y}_{\mathrm{ols}}^{2}]\\
&amp; =2\mathbb{E}[Y\hat{Y}_{\mathrm{ols}}]-\left(\mathbb{E}Y\right)^{2}-\mathbb{E}[\hat{Y}_{\mathrm{ols}}^{2}]\\
&amp; =\operatorname{Cov}(Y,\hat{Y}_{\mathrm{ols}})+\mathbb{E}[Y\hat{Y}_{\mathrm{ols}}]-\mathbb{E}[\hat{Y}_{\mathrm{ols}}^{2}]\\
&amp; =\operatorname{Var}(\hat{Y}_{\mathrm{ols}}). \blacksquare
\end{aligned}
$$</p>
<p>Putting all of the above results together, we arrive at the identities mentioned in the <strong>Introduction and Results</strong> section.</p>
<h2 id="synthetic-example">Synthetic example</h2>
<p>We double-check the claims via a synthetic example below:</p>
<pre><code data-lang="python">np.random.seed(1)

N = 1_000
p = 5

X = np.random.randn(N, p)
Y = np.random.randn(N)

β = sm.OLS(Y, sm.add_constant(X)).fit().params
Yhat = β[0] + X @ β[1:]

R2 = 1.0 - ((Y - Yhat) ** 2).mean() / Y.var()
CovYYhat = np.cov(Y, Yhat, bias=True)[0, 1]
CovXY = np.cov(X, Y, rowvar=False, bias=True)[:-1, -1]
Corr2 = np.corrcoef(Yhat, Y)[0, 1] ** 2

# On the training set, the R squared satisfies the identities
np.testing.assert_allclose(R2, Yhat.var() / Y.var())
np.testing.assert_allclose(R2, CovYYhat / Y.var())
np.testing.assert_allclose(R2, CovXY @ β[1:] / Y.var())
np.testing.assert_allclose(R2, Corr2)

X_H = np.random.randn(N, p)
Y_H = np.random.randn(N)

Yhat_H = β[0] + X @ β[1:]

R2_H = 1.0 - ((Y_H - Yhat_H) ** 2).mean() / Y_H.var()
CovYYhat_H = np.cov(Y_H, Yhat_H, bias=True)[0, 1]
CovXY_H = np.cov(X_H, Y_H, rowvar=False, bias=True)[:-1, -1]
Corr2_H = np.corrcoef(Yhat_H, Y_H)[0, 1] ** 2

# On the test set, the R squared does not satisfy the identities
assert not np.isclose(R2_H, Yhat_H.var() / Y.var())
assert not np.isclose(R2_H, CovYYhat_H / Y_H.var())
assert not np.isclose(R2_H, CovXY_H @ β[1:] / Y_H.var())
assert not np.isclose(R2_H, Corr2_H)
</code></pre>

</div>
  <footer>Powered by <a href="https://getzola.org/">Zola</a></footer>
</body>

</html>