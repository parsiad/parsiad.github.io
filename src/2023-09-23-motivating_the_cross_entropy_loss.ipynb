{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fef12b24-cc16-469f-9827-560cb14a43e6",
   "metadata": {},
   "source": [
    "---\n",
    "date: 2023-09-23 12:00:00-0800\n",
    "layout: post\n",
    "title: Motivating the cross-entropy loss\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d6b75-f789-4810-878c-43e046d04bc4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In machine learning, the cross-entropy loss is frequently introduced without explicitly emphasizing its underlying connection to the likelihood of a categorical distribution.\n",
    "Understanding this link can greatly enhance one's grasp of the loss and is the topic of this short post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddcdfcb-8d96-4f76-bb58-76415e45e18f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* [maximum likelihood estimator (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1574ab-52ea-46d5-b0d6-8d3e8c86d31d",
   "metadata": {},
   "source": [
    "## Categorical distribution likelihood\n",
    "\n",
    "Consider an experiment in which we roll a (not necessarily fair) $K$-sided die.\n",
    "The result of this roll is an integer between $1$ and $K$ (inclusive) corresponding to the faces of the die. Let $q(k)$ be the probability of seeing the $k$-th face.\n",
    "What we have described here, in general, is a categorical random variable: a random variable which takes one of a finite number of values.\n",
    "Repeating this experiment multiple times yields IID random variables $X_{1},\\ldots,X_{N}\\sim\\operatorname{Categorical}(q)$.\n",
    "\n",
    "Performing this experiment a finite number of times $N$ does not allow us to introspect $q$ precisely, but it does allow us to estimate it.\n",
    "One way to approximate $q(k)$ is by counting the number of times the die face $k$ was observed and normalizing the result: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\\tag{1}\\label{eq:empirical_pmf}\n",
    "p(k)=\\frac{1}{N}\\sum_{n}[X_{n}=k]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $[\\cdot]$ is the [Iverson bracket](https://en.wikipedia.org/wiki/Iverson_bracket). Since $Y_{n}=[X_{n}=k]$ is itself a random variable (an indicator random variable), the law of large numbers tells us that $p(k)$ converges (a.s.) to $\\mathbb{E}Y_{1}=\\mathbb{P}(X_{n}=k)=q(k)$.\n",
    "\n",
    "The likelihood of $q$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q)=\\prod_{n}\\prod_{k}q(k)^{[X_{n}=k]}=\\prod_{k}q(k)^{\\sum_{n}[X_{n}=k]}=\\prod_{k}q(k)^{Np(k)}\n",
    "$$\n",
    "\n",
    "and hence its log-likelihood is\n",
    "\n",
    "$$\n",
    "\\ell(q)=\\log\\mathcal{L}(q)=\\sum_{k}Np(k)\\log q(k)\\propto\\sum_{k}p(k)\\log q(k).\n",
    "$$\n",
    "\n",
    "**Proposition**. The MLE for the parameter of the categorical distribution is the empirical probability mass function \\eqref{eq:empirical_pmf}.\n",
    "\n",
    "*Proof*. Consider the program\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{q} & -\\ell(q)\\\\\n",
    "\\text{subject to} & \\sum_{k}q(k)-1=0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The [Karush--Kuhn--Tucker stationarity condition](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) is\n",
    "\n",
    "$$\n",
    "-\\frac{p(j)}{q(j)}+\\lambda=0.\n",
    "$$\n",
    "\n",
    "In other words, the MLE $\\hat{q}$ is a multiple of $p$.\n",
    "Since the MLE needs to be a probability vector, $\\hat{q} = p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf2983-d193-4aed-9dc1-9af618d070a6",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "The cross-entropy between $q$ relative to $p$ is\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\mathbb{E}_{X \\sim p} [ \\log q(X) ].\n",
    "$$\n",
    "\n",
    "The choice of logarithm base yields different units:\n",
    "* base 2: [bits](https://en.wikipedia.org/wiki/Bit)\n",
    "* base e: [nats](https://en.wikipedia.org/wiki/Nat_(unit))\n",
    "* base 10: [hartleys](https://en.wikipedia.org/wiki/Hartley_(unit))\n",
    "\n",
    "When $p$ and $q$ are probability mass functions (PMFs), the cross-entropy reduces to\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\sum_x p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "which is exactly the (negation of the) log-likelihood we encountered above.\n",
    "As such, one can intuit that minimizing $q$ in the cross-entropy yields a distribution that is similar to $p$.\n",
    "In other words, **the cross-entropy is an asymmetric measure of dissimilarity between $q$ and $p$.**\n",
    "\n",
    "The [Kullback--Leibler (KL) divergence](https://en.m.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is another such measure:\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{KL}}(p\\Vert q)\n",
    "=\\mathbb{E}_{p}\\left[\\log\\frac{p(X)}{q(X)}\\right]\n",
    "=H(p,q) - H(p,p).\n",
    "$$\n",
    "\n",
    "Minimizing the KL divergence is the same as minimizing the cross-entropy, but the KL divergence satisfies some nice properties that one would expect of a measure of dissimilarity.\n",
    "In particular,\n",
    "1. $D_{\\mathrm{KL}}(p\\Vert q) \\geq 0$\n",
    "2. $D_{\\mathrm{KL}}(p\\Vert p) = 0$\n",
    "\n",
    "We proved the first inequality for PMFs by showing that the choice of $q = p$ maximizes the cross-entropy.\n",
    "The second inequality is trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5417a1-e995-471b-93b3-125f80237543",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "Statistical classification is the problem of mapping each input datum $x \\in \\mathcal{X}$ to a class label $y = 1, \\ldots, K$.\n",
    "For example, in the [CIFAR-10](https://en.wikipedia.org/wiki/CIFAR-10) classification task, each $x$ is a 32x32 color image and each $K = 10$ corresponding to ten distinct classes (e.g., airplanes, cats, trucks).\n",
    "\n",
    "A common parametric estimator for image classification tasks such as CIFAR-10 is a [neural network](https://en.wikipedia.org/wiki/Neural_network): a differentiable map $f: \\mathcal{X} \\rightarrow \\mathbb{R}^K$.\n",
    "Note, in particular, that the network outputs a vector of real numbers.\n",
    "These are typically transformed to probabilities by way of the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) $\\sigma$.\n",
    "In other words, for input $x$, $\\hat{y} = \\sigma(f(x))$ is a probability vector of size $K$.\n",
    "The $k$-th element of this vector is the \"belief\" that the network assigns to $x$ being a member of class $k$.\n",
    "\n",
    "Given a set of observations $\\mathcal{D} = \\{(x_1, y_1), \\ldots, (x_N, y_N)\\}$, the cross-entropy loss for this task is\n",
    "\n",
    "$$\n",
    "L(\\mathcal{D}) = \\frac{1}{N}\\sum_{n}H(p_{n},q_{n})\n",
    "$$\n",
    "\n",
    "where $q_{n}=\\sigma(f(x_{n}))$ and $p_{n}$ is the probability mass\n",
    "function which places all of its mass on $y_{n}$.\n",
    "Expanding this, we obtain what is to some the more familiar representation\n",
    "\n",
    "$$\n",
    "L(\\mathcal{D}) = -\\frac{1}{N}\\sum_{n}[\\log\\sigma(f(x_{n}))]_{y_{n}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb643cf2-2399-4ef3-a5b1-5858da16d621",
   "metadata": {},
   "source": [
    "## See also\n",
    "\n",
    "* PyTorch [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "* Keras [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
