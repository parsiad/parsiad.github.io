{
 "cells": [
  {
   "cell_type": "raw",
   "id": "01f6a610-4335-4a6f-bc37-8badac2f872e",
   "metadata": {},
   "source": [
    "---\n",
    "date: 2024-09-02 12:00:00-0500\n",
    "layout: post\n",
    "title: R squared and multiple regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57626c7-52ab-49f9-99fc-61541d5027ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Introduction and Results\n",
    "\n",
    "The *R squared* of a predictor $\\hat{Y}$ relative to a target $Y$ is the proportion of the variation in the target that is explained by that predictor.\n",
    "In this short note, we introduce the R squared in its most general form.\n",
    "\n",
    "We then turn our attention to the predictor $\\hat{Y}_{\\mathrm{ols}} = \\hat{\\beta}_0 + X^\\intercal \\hat{\\beta}_1$ constructed by [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) (OLS).\n",
    "In this case, we prove that the R squared satisfies\n",
    "\n",
    "$$\n",
    "\\boxed{R^{2}(Y,\\hat{Y}_{\\mathrm{ols}})\n",
    "=\\frac{\\operatorname{Cov}(Y,\\hat{Y}_{\\mathrm{ols}})}{\\operatorname{Var}(Y)}\n",
    "=\\frac{\\operatorname{Cov}(X,Y)^\\intercal \\hat{\\beta}_1}{\\operatorname{Var}(Y)}\n",
    "=\\frac{\\operatorname{Var}(\\hat{Y}_{\\mathrm{ols}})}{\\operatorname{Var}(Y)}\n",
    "=\\operatorname{Corr}(Y,\\hat{Y}_{\\mathrm{ols}})^{2}}\n",
    "$$\n",
    "\n",
    "where the covariance $\\operatorname{Cov}(X, Y)$ is the vector whose entries are $\\operatorname{Cov}(X_i, Y)$.\n",
    "\n",
    "In addition, when the covariance matrix $\\operatorname{Cov}(X, X)$ is nonsingular, we can substitute the unique representation for $\\hat{\\beta}_1$ to get\n",
    "\n",
    "$$\n",
    "\\boxed{R^{2}(Y,\\hat{Y}_{\\mathrm{ols}})\n",
    "=\\frac{\\operatorname{Cov}(X,Y)^\\intercal\\operatorname{Cov}(X,X)^{-1}\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(Y)}.}\n",
    "$$\n",
    "\n",
    "In the case of a one dimensional $X$, the above is the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between $X$ and $Y$.\n",
    "\n",
    "These identities may be folklore in the following sense: I believe they are well-known but that a proof, outside of the one dimensional case, is hard to find.\n",
    "Note that in the one dimensional case, OLS (with the intercept term $\\hat{\\beta}_0$) is simply called [simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456418c7-62fa-4cc9-b42a-3dc7c20f6fc2",
   "metadata": {},
   "source": [
    "## R squared\n",
    "\n",
    "As mentioned above, the R squared is a fraction of variation.\n",
    "A natural notion of variation between two random quantities is the mean squared error:\n",
    "\n",
    "**Definition (Mean squared error).**\n",
    "The *mean squared error* (MSE) between real random variables $A$ and $B$ is\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(A, B) \\equiv \\mathbb{E}[(A - B)^2].\n",
    "$$\n",
    "\n",
    "We are now ready to define R squared:\n",
    "\n",
    "**Definition (R squared).**\n",
    "Let $Y$ and $\\hat{Y}$ be real random variables. The *R squared* of $\\hat{Y}$ relative to $Y$ is\n",
    "\n",
    "$$\n",
    "R^2(Y, \\hat{Y})\n",
    "\\equiv 1 - \\frac{\\operatorname{MSE}(Y,\\hat{Y})}{\\operatorname{Var}(Y)}\n",
    "$$\n",
    "\n",
    "Written in the above form, the R squared has an intuitive definition: it is equal to one minus the fraction of unexplained variation (note that the variance itself is a special case of the MSE since $\\operatorname{Var}(Y) = \\operatorname{MSE}(Y,\\mathbb{E}Y)$).\n",
    "\n",
    "The following facts can be verified by direct substitution into the definition of R squared:\n",
    "\n",
    "**Fact.**\n",
    "*A \"perfect\" prediction (i.e., $\\operatorname{MSE}(Y,\\hat{Y})=0$) has an R squared of one.*\n",
    "\n",
    "**Fact.**\n",
    "*A prediction of the mean (i.e., $\\hat{Y}=\\mathbb{E}Y)$ has an R squared of zero.*\n",
    "\n",
    "The above give us an upper bound (one) and a \"weak\" lower bound (zero) for the R squared.\n",
    "The lower bound is weak in the sense that it is possible to produce predictors that have a negative R squared but that such predictors are typically pathological as they are worse (in the sense of R squared) than simply predicting the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e14b2-deb7-42e4-83d2-b204295ca46f",
   "metadata": {},
   "source": [
    "## Ordinary least squares\n",
    "\n",
    "Below, we define OLS with an intercept term.\n",
    "\n",
    "**Definition (Ordinary least squares).**\n",
    "Given a real random variable $Y$ and a real random vector $X$, let\n",
    "\n",
    "$$\n",
    "\\hat{Y}_{\\mathrm{ols}}\\equiv\\hat{\\beta}_{0}+X^{\\intercal}\\hat{\\beta}_{1}\n",
    "$$\n",
    "\n",
    "be the *ordinary least squares* (OLS) predictor where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_{0} & =\\mathbb{E}Y-\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}\\nonumber \\\\\n",
    "\\mathbb{E}[XX^{\\intercal}]\\hat{\\beta}_{1} & =\\mathbb{E}[XY]-\\mathbb{E}X\\hat{\\beta}_{0}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Remark*.\n",
    "We could have also defined $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ by the equivalent system\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_{0} & =\\mathbb{E}Y-\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}\\nonumber \\\\\n",
    "\\operatorname{Cov}(X, X)\\hat{\\beta}_{1} & =\\operatorname{Cov}(X, Y).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Remark*.\n",
    "Thus far, we have only used a single probability measure: that which is implied by the expectation $\\mathbb{E}$.\n",
    "In data-driven applications, it is standard practice to fit the coefficients on a subset of data (also known as the [training set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Training_data_set)) while \"holding out\" the remainder of the data (also known as the [test set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets#Test_data_set)) to evaluate the quality of the fit.\n",
    "This results in two distinct expectations: $\\mathbb{E}$ and $\\mathbb{E}_H$ ($H$ for \"hold out\").\n",
    "In the context of R squared, this results in two natural quantities: $R^2$ and $R_H^2$ where the latter is defined by replacing all expectations $\\mathbb{E}$ in the MSE and variance by $\\mathbb{E}_H$.\n",
    "We stress that the results of this section apply only to the former.\n",
    "Put more succinctly, **on the test set, R squared is not guaranteed to be equal to the square of the correlation between the predictor and target** for the OLS predictor.\n",
    "\n",
    "To establish that on the training set, R squared is equal to the square of the correlation between the predictor and target for the OLS predictor, we use a series of smaller results:\n",
    "\n",
    "**Lemma 1.**\n",
    "*The OLS predictor is unbiased.*\n",
    "\n",
    "*Proof*.\n",
    "Direct computation yields\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\hat{Y}_{\\mathrm{ols}}\\equiv\\hat{\\beta}_{0}+\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}=\\mathbb{E}Y-\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}+\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}=\\mathbb{E}Y.\\blacksquare\n",
    "$$\n",
    "\n",
    "**Lemma 2.**\n",
    "*The second moment of the OLS predictor satisfies*\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{Y}_{\\mathrm{ols}}^{2}]\n",
    "= \\mathbb{E}[Y\\hat{Y}_{\\mathrm{ols}}]\n",
    "= \\left(\\mathbb{E}Y\\right)^2 + \\operatorname{Cov}(X, Y)^\\intercal \\hat{\\beta}_1.\n",
    "$$\n",
    "\n",
    "*Proof*.\n",
    "Direct computation along with the definitions of $\\hat{\\beta}_0$ and $\\mathbb{E}[XX^{\\intercal}]\\hat{\\beta}_1$ reveal\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[Y\\hat{Y}_{\\mathrm{ols}}] & =\\mathbb{E}\\left[Y\\left(\\hat{\\beta}_{0}+X^{\\intercal}\\hat{\\beta}_{1}\\right)\\right]\\\\\n",
    " & =\\mathbb{E}\\left[Y\\hat{\\beta}_{0}+X^{\\intercal}Y\\hat{\\beta}_{1}\\right]\\\\\n",
    " & =\\mathbb{E}Y\\hat{\\beta}_{0}+\\mathbb{E}[X^{\\intercal}Y]\\hat{\\beta}_{1}\\\\\n",
    " & =\\mathbb{E}Y\\left(\\mathbb{E}Y-\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}\\right)+\\mathbb{E}[X^{\\intercal}Y]\\hat{\\beta}_{1}\\\\\n",
    " & =\\left(\\mathbb{E}Y\\right)^{2}-\\mathbb{E}X^{\\intercal}\\mathbb{E}Y\\hat{\\beta}_{1}+\\mathbb{E}[X^{\\intercal}Y]\\hat{\\beta}_{1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{Y}_{\\mathrm{ols}}^{2}] & =\\mathbb{E}\\left[\\left(\\hat{\\beta}_{0}+X^{\\intercal}\\hat{\\beta}_{1}\\right)^{2}\\right]\\\\\n",
    " & =\\mathbb{E}\\left[\\hat{\\beta}_{0}^{2}+2\\hat{\\beta}_{0}X^{\\intercal}\\hat{\\beta}_{1}+\\hat{\\beta}_{1}^{\\intercal}XX^{\\intercal}\\hat{\\beta}_{1}\\right]\\\\\n",
    " & =\\hat{\\beta}_{0}^{2}+2\\hat{\\beta}_{0}\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}+\\hat{\\beta}_{1}^{\\intercal}\\mathbb{E}[XX^{\\intercal}]\\hat{\\beta}_{1}\\\\\n",
    " & =\\hat{\\beta}_{0}^{2}+2\\hat{\\beta}_{0}\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}+\\left(\\mathbb{E}[X^{\\intercal}Y]-\\hat{\\beta}_{0}\\mathbb{E}X^{\\intercal}\\right)\\hat{\\beta}_{1}\\\\\n",
    " & =\\left(\\mathbb{E}Y-\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}\\right)^{2}+\\left(\\mathbb{E}Y-\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}\\right)\\mathbb{E}X^{\\intercal}\\hat{\\beta}_{1}+\\mathbb{E}\\left[X^{\\intercal}Y\\right]\\hat{\\beta}_{1}\\\\\n",
    " & =\\left(\\mathbb{E}Y\\right)^{2}-\\mathbb{E}X^{\\intercal}\\mathbb{E}Y\\hat{\\beta}_{1}+\\mathbb{E}[X^{\\intercal}Y]\\hat{\\beta}_{1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "as desired. $\\blacksquare$\n",
    "\n",
    "**Corollary 3.**\n",
    "*The variance of the OLS predictor satisfies*\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{Y}_{\\mathrm{ols}})\n",
    "=\\operatorname{Cov}(Y,\\hat{Y}_{\\mathrm{ols}}).\n",
    "$$\n",
    "\n",
    "*Proof*.\n",
    "Applying Lemmas 1 and 2,\n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(Y,\\hat{Y}_{\\mathrm{ols}})=\\mathbb{E}[Y\\hat{Y}_{\\mathrm{ols}}]-\\mathbb{E}Y\\mathbb{E}\\hat{Y}_{\\mathrm{ols}}=\\mathbb{E}[\\hat{Y}_{\\mathrm{ols}}^{2}]-\\left(\\mathbb{E}\\hat{Y}_{\\mathrm{ols}}\\right)^{2}=\\operatorname{Var}(\\hat{Y}_{\\mathrm{ols}}).\\blacksquare\n",
    "$$\n",
    "\n",
    "**Corollary 4.**\n",
    "*The MSE of the OLS predictor can be decomposed as*\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(Y,\\hat{Y}_{\\mathrm{ols}})\n",
    "=\\operatorname{Var}(Y)-\\operatorname{Var}(\\hat{Y}_{\\mathrm{ols}}).\n",
    "$$\n",
    "\n",
    "*Proof*.\n",
    "Applying Lemmas 1 and 2 and Corollary 3,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y)-\\operatorname{MSE}(Y,\\hat{Y}_{\\mathrm{ols}}) & =\\mathbb{E}[Y^{2}]-\\left(\\mathbb{E}Y\\right)^{2}-\\mathbb{E}[Y^{2}]+2\\mathbb{E}[Y\\hat{Y}_{\\mathrm{ols}}]-\\mathbb{E}[\\hat{Y}_{\\mathrm{ols}}^{2}]\\\\\n",
    " & =2\\mathbb{E}[Y\\hat{Y}_{\\mathrm{ols}}]-\\left(\\mathbb{E}Y\\right)^{2}-\\mathbb{E}[\\hat{Y}_{\\mathrm{ols}}^{2}]\\\\\n",
    " & =\\operatorname{Cov}(Y,\\hat{Y}_{\\mathrm{ols}})+\\mathbb{E}[Y\\hat{Y}_{\\mathrm{ols}}]-\\mathbb{E}[\\hat{Y}_{\\mathrm{ols}}^{2}]\\\\\n",
    " & =\\operatorname{Var}(\\hat{Y}_{\\mathrm{ols}}). \\blacksquare\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Putting all of the above results together, we arrive at the identities mentioned in the **Introduction and Results** section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005e305-6ab2-465d-bab5-5fe3539bd0bc",
   "metadata": {},
   "source": [
    "## Synthetic example\n",
    "\n",
    "We double-check the claims via a synthetic example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcdd900-3be4-4b87-b02d-bd7df1eb5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "N = 1_000\n",
    "p = 5\n",
    "\n",
    "X = np.random.randn(N, p)\n",
    "Y = np.random.randn(N)\n",
    "\n",
    "β = sm.OLS(Y, sm.add_constant(X)).fit().params\n",
    "Yhat = β[0] + X @ β[1:]\n",
    "\n",
    "R2 = 1. - ((Y - Yhat)**2).mean() / Y.var()\n",
    "Corr2 = np.corrcoef(Yhat, Y)[0, 1]**2\n",
    "\n",
    "# On the training set, R2 is equal to the correlation squared\n",
    "np.testing.assert_allclose(R2, Corr2)\n",
    "\n",
    "X_H = np.random.randn(N, p)\n",
    "Y_H = np.random.randn(N)\n",
    "\n",
    "Yhat_H = β[0] + X @ β[1:]\n",
    "\n",
    "R2_H = 1. - ((Y_H - Yhat_H)**2).mean() / Y_H.var()\n",
    "Corr2_H = np.corrcoef(Yhat_H, Y_H)[0, 1]**2\n",
    "\n",
    "# On the test set, R2 is NOT equal to the correlation squared\n",
    "assert not np.isclose(R2_H, Corr2_H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
